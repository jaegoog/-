{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN_classification for Sentence.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNqgeCzkDFiyvrVC0/h68L+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"Z4FaPCR4Mu2_"},"source":["pip install tensorflow-addons"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rgzbnt7yMxQS"},"source":["! git clone https://github.com/e9t/nsmc.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pn6HPnyRMyuP"},"source":["!sudo apt-get install g++ openjdk-7-jdk # Install Java 1.7+\n","# !sudo apt-get install python-dev; pip install konlpy     # Python 2.x\n","!sudo apt-get install python3-dev; pip3 install konlpy   # Python 3.x\n","!sudo apt-get install curl\n","!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WljspYjAMrGO"},"source":["import pandas as pd\n","import re\n","import numpy as np\n","\n","from tqdm import tqdm\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","\n","from tensorflow import keras\n","from gensim.models import Word2Vec\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Input, Dense, Embedding, BatchNormalization, Flatten, Concatenate, Dropout, Conv1D, MaxPooling1D\n","from tensorflow.keras import datasets, layers, Model\n","\n","\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.metrics import binary_accuracy\n","from tensorflow.keras.losses import binary_crossentropy\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","from gensim import models\n","import gensim\n","\n","from pykospacing import Spacing\n","spacing = Spacing()\n","\n","from konlpy.tag import Mecab, Okt\n","mecab = Mecab()\n","okt = Okt()\n","\n","import matplotlib.pyplot as plt\n","from wordcloud import WordCloud\n","import seaborn as sns\n","\n","import random\n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","\n","%matplotlib inline "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"srueoeAXLv81"},"source":["\n","train = pd.read_csv(\"/content/drive/MyDrive/머신러닝, 딥러닝 논문/Convolutional_Neural_Networks_for_Sentence_Classification/train_spacing.csv\", encoding='utf-8')\n","test = pd.read_csv(\"/content/drive/MyDrive/머신러닝, 딥러닝 논문/Convolutional_Neural_Networks_for_Sentence_Classification/test_spacing.csv\", encoding='utf-8')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DestDEPkMPlH"},"source":["\n","train.fillna('NaN', inplace=True) \n","test.fillna('NaN', inplace=True) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q-U72YN3NSfa"},"source":["# **전처리**"]},{"cell_type":"code","metadata":{"id":"yZtegiLsMSDB"},"source":["def clean_text(texts):\n","    corpus = []\n","    for i in tqdm(range(0, len(texts))):\n","        \n","        review = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '',str(texts[i])) #remove punctuation\n","        review = re.sub(r'\\d+','', str(texts[i]))# remove number\n","        review = review.lower() #lower case\n","        review = re.sub(r'\\s+', ' ', review) #remove extra space\n","        review = re.sub('[-=+,#:;//●<>▲\\?:^$.☆!★()Ⅰ@*\\\"※~>`\\'…》→←·]', ' ', review)\n","        review = re.sub(r\"^\\s+\", '', review) #remove space from start\n","        review = re.sub(r'\\s+$', '', review) #remove space from the end\n","        review = re.sub(\"[一-龥]\",'', review) # remove hanja\n","        corpus.append(review)\n","    return corpus\n","\n","\n","train.data_text = list(clean_text(train.document))\n","test.data_text = list(clean_text(test.document))\n","clear_text1, clear_text2=  [], []\n","\n","\n","for i in range(len(train.data_text)):\n","    clear_text1.append(str(train.data_text[i]).replace('\\\\n', ''))\n","\n","for i in range(len(test.data_text)):\n","    clear_text2.append(str(test.data_text[i]).replace('\\\\n', ''))\n","\n","\n","train['clear_text'] = clear_text1\n","test['clear_text'] = clear_text2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1j2QN3DuNWLm"},"source":["\"\"\"\n","{'Adjective': '형용사',\n"," 'Adverb': '부사',\n"," 'Alpha': '알파벳',\n"," 'Conjunction': '접속사',\n"," 'Determiner': '관형사',\n"," 'Eomi': '어미',\n"," 'Exclamation': '감탄사',\n"," 'Foreign': '외국어, 한자 및 기타기호',\n"," 'Hashtag': '트위터 해쉬태그',\n"," 'Josa': '조사',\n"," 'KoreanParticle': '(ex: ㅋㅋ)',\n"," 'Noun': '명사',\n"," 'Number': '숫자',\n"," 'PreEomi': '선어말어미',\n"," 'Punctuation': '구두점',\n"," 'ScreenName': '트위터 아이디',\n"," 'Suffix': '접미사',\n"," 'Unknown': '미등록어',\n"," 'Verb': '동사'}\n","\"\"\"\n","def pos(df):\n","    try:\n","        pos_text = []\n","        for sentence in df:\n","            sentence = okt.pos(sentence, stem=True, norm=True) # norm : If True, normalize tokens. stem : If True, stem tokens. join : If True, returns joined sets of morph and tag.\n","\n","            temp = []\n","            for (word, pos) in sentence:\n","                if pos not in ['Suffix', 'Josa', 'Determiner', 'Modifier', 'KoreanParticle', 'Exclamation', 'Conjunction'] : # 접미사, 조사, 관형사,  Modifier?, 감탄사, 접속사\n","                    temp.append(word)\n","            pos_text.append(' '.join(temp)) \n","        return pos_text\n","    \n","    except:\n","        pass\n","\n","train[\"pos_text\"] = pos(train.clear_text)\n","test[\"pos_text\"] = pos(test.clear_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N8nRDvThNfLS"},"source":["train_corpus = [text for text in tqdm(train.pos_text)]\n","test_corpus = [text for text in tqdm(test.pos_text)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VXgkbG0KNnZp"},"source":["\n","* ## **불용어 처리**\n","\n"]},{"cell_type":"code","metadata":{"id":"-usrP0tENhMU"},"source":["STOPWORDSPATH =\"/content/drive/MyDrive/Colab Notebooks/한국어불용어100.txt\"\n","stopwords = [\n","             '로', '은', '는', '였', '엿', '고', '를', '을', '에', '도', '의', '입니다.', 'ㅋ' ,'ㅋㅋ', 'ㅋㅋㅋ', 'ㅎ', 'ㅎㅎ','ㅎㅎㅎ', '어요', '께', '게', '과', '왔', 'ㄷ', 'ㅠ', 'ㅠㅠ', '네요', '거', '더라구요', '었', '에서', '으로', '봤', '는데', '겟', '겠', '았', '앗', '밋', '와', '뭐', '습니다', '된', 'ㅇ','ㅇㅇ', '합니다', '했', '아요', '요',\n","             '입니다', '듯', '지만', '인데', '까지', '로써', '로서', '된다', '임', '그리고', '그래서', '던', '에요', 'ㅜ','ㅜㅜ', '해', '요', '냐', '한다', '셈', \"끼리\", '_', '더군요','니까', '에게', '%', '라는','으루', '는지', '잖아요', '잖아', '세요', '네여', \n","             '하다', '나다', '돼다', '되다', '이다', '싶다', '있다', '어떻다', '같다', '것', '그', '데'\n","\n","                    ]\n","with open(STOPWORDSPATH) as f:\n","    for line in f:\n","        line = line.split('\\t')\n","\n","        stopwords.append(line[0])\n","        # print(line[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NIUhEQ_pNkgG"},"source":["def clear_sw(s):\n","    sentence = s.split()\n","    temp = []\n","    for token in sentence:\n","        if token not in stopwords:\n","            temp.append(token)\n","    temp = ' '.join(temp)\n","    return temp\n","clear_corpus_train = []\n","clear_corpus_test = []\n","# for sentence in train_corpus:\n","for sentence in train_corpus:\n","    clear_corpus_train.append(clear_sw(sentence)) \n","for sentence in test_corpus:\n","    clear_corpus_test.append(clear_sw(sentence)) \n","\n","train_label = np.array(train.label)\n","test_label = np.array(test.label, dtype=np.int32)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FhWcOItHNRAs"},"source":["# **Tokenizing, padding**"]},{"cell_type":"code","metadata":{"id":"Jc9l0-JYOVl4"},"source":["max_length = 42\n","# model_type = 'CNN-static'\n","# train_sentences = datastore.document.values\n","# train_labels = datastore.label.values\n","# train_ids = datastore.id.values\n","\n","# test_sentences = test_data.document.values\n","# test_labels = test_data.label.values\n","# test_ids = test_data.id.values\n","\n","#############################################\n","#############################################\n","\n","tokenizer = Tokenizer(oov_token='<oov>')\n","tokenizer.fit_on_texts(clear_corpus_train)\n","\n","train_word_index = tokenizer.word_index\n","train_vocabulary_inv = tokenizer.index_word# {v: k for k, v in tokenizer.word_index.items()}\n","\n","train_sequences = tokenizer.texts_to_sequences(clear_corpus_train)\n","test_sequences = tokenizer.texts_to_sequences(clear_corpus_test)\n","\n","train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n","test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CAL6ycOwOZS7"},"source":["# **pre-trained 모델 적용(fasttext)**"]},{"cell_type":"code","metadata":{"id":"rcfodLYIOht5"},"source":["#-*- coding: ISO-8859-1' -*-\n","# encoding = 'ISO-8859-1'\n","def load_dic(dic_file) :\n","    embeddings_index = dict()\n","    f = open(dic_file,  encoding = 'ISO-8859-1')\n","    for i, line in enumerate(f):\n","        if i == 0 : continue\n","    try :\n","        values = line.split()\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        embeddings_index[word] = coefs\n","    except :\n","        print(i)\n","        print(values)\n","    f.close()\n","\n","    return embeddings_index\n","\n","embeddings_index = load_dic('/content/drive/MyDrive/pre_trained_data/cc.ko.300.vec')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"djT_JP0WOlwX"},"source":["def gen_embedding_matrix(vocab, embeddings_index, embedding_size) :\n","    vocabulary_size = len(vocab) + 1\n","\n","    embedding_matrix = np.zeros((vocabulary_size, embedding_size))\n","    for word, index in vocab.items():\n","        if index > vocabulary_size - 1:\n","            break\n","        else:\n","            embedding_vector = embeddings_index.get(word)\n","            if embedding_vector is not None:\n","                embedding_matrix[index] = embedding_vector\n","    return embedding_matrix\n","\n","embedding_matrix = gen_embedding_matrix(train_word_index, embeddings_index, embedding_size=200)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UEcDsRdMOtri"},"source":["# **모델 설정 및 layer 쌓기**"]},{"cell_type":"code","metadata":{"id":"xZ08JgQoOw4a"},"source":["vocab_size = len(train_word_index) +1 # 40821\n","embedding_dim = 200\n","max_length = max_length \n","\n","filter_sizes = (3, 4, 5)\n","hidden_dims = 100\n","\n","num_filters = 100\n","\n","input_shape = (max_length, ) # 18, 200\n","\n","cnn_model_input = Input(shape = input_shape)\n","cnn_embedding = Embedding(vocab_size , embedding_dim, input_length=max_length,weights=[embedding_matrix], trainable=True)(cnn_model_input) \n","cnn_Dropout_1 = Dropout(0.2)(cnn_embedding) \n","\n","cnn_Convs_block = []\n","for size in filter_sizes:\n","    cnn_Conv_1 = Conv1D(filters=num_filters, kernel_initializer='glorot_normal', kernel_size=size, padding='valid', activation='relu', strides=1)(cnn_Dropout_1)\n","    \n","    cnn_MaxP = MaxPooling1D(pool_size=2,padding='valid')(cnn_Conv_1)\n","    cnn_Conv_1 = Conv1D(64, 3, kernel_initializer='glorot_normal', padding='same', activation='relu', strides=1)(cnn_MaxP)\n","    cnn_MaxP = MaxPooling1D(pool_size=2,padding='valid')(cnn_Conv_1)\n","    cnn_Flatten = Flatten()(cnn_MaxP)\n","    cnn_Convs_block.append(cnn_Flatten)\n","\n","concate_cnn_layers = Concatenate()(cnn_Convs_block) if len(cnn_Convs_block) > 1 else cnn_Convs_block[0]\n","\n","cnn_Dropout_2 = Dropout(0.5)(concate_cnn_layers)\n","cnn_Dense_1 = Dense(1024, kernel_initializer='glorot_normal', activation='relu')(cnn_Dropout_2)\n","\n","cnn_Dropout_3 = Dropout(0.5)(concate_cnn_layers)\n","cnn_Dense_2 = Dense(256, activation='relu')(cnn_Dropout_3)\n","\n","cnn_Dropout_4 = Dropout(0.2)(cnn_Dense_2)\n","cnn_Dense_3 = Dense(64, activation='relu')(cnn_Dropout_4)\n","\n","cnn_Dropout_5 = Dropout(0.2)(cnn_Dense_3)\n","cnn_Dense_4 = Dense(16, activation='relu')(cnn_Dropout_5)\n","\n","model_output = Dense(1, activation='sigmoid')(cnn_Dense_4)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pA_l5p-gO_Gi"},"source":["* ## optimizer : Radam"]},{"cell_type":"code","metadata":{"id":"wf_lowyMO7H3"},"source":["model = Model(cnn_model_input, model_output)\n","model.compile(loss='binary_crossentropy', \n","              optimizer= tfa.optimizers.RectifiedAdam(learning_rate=7.0e-4, total_steps = 5000, warmup_proportion=0.2, min_lr=1e-5, epsilon=1e-08, clipnorm=1.0), \n","              metrics=['accuracy'])\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HPTh2oMcO-Z5"},"source":["model.fit(train_padded, train_label, batch_size=512, epochs=20,\n","          validation_split=0.2, verbose=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ysFbjwAVPQfx"},"source":["# **결과**"]},{"cell_type":"code","metadata":{"id":"qKsYKO6xPJqq"},"source":["pred = model.predict(test_padded)\n","result = []\n","for i in pred:\n","    if i>=0.5:\n","        result.append(1)\n","    else:\n","        result.append(0)\n","print(f\"{ round(sum(result==test['label'])/len(test)*100, 4) }%\")"],"execution_count":null,"outputs":[]}]}